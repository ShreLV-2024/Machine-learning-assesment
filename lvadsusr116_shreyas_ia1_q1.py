# -*- coding: utf-8 -*-
"""LVADSUSR116_Shreyas_IA1-Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEskFZm8R-FpK10O-vO4z-JuW7wooq2B
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.tree import DecisionTreeClassifier

pd.options.display.max_rows = 1500

# Q1
df = pd.read_csv('expenses.csv')

df

for i in df.columns:
    print(df[df[i].isnull()])

# There are no null values present

# Lets check outliers for bmi and charges
# 1. Bmi
Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)

IQR = Q3-Q1

lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR

# Outlier datapoints
df[(df['bmi'] < lower_limit) | (df['bmi'] > upper_limit)]

# 2. Charges
Q1 = df['charges'].quantile(0.25)
Q3 = df['charges'].quantile(0.75)

IQR = Q3-Q1

lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR

# Outlier datapoints
df[(df['charges'] < lower_limit) | (df['charges'] > upper_limit)]





# Basic visualizations
sns.scatterplot(data = df, x = 'age', y='charges', hue = 'sex')

sns.scatterplot(data = df, x = 'bmi', y='age', hue = 'region')

sns.distplot(x = df['charges'], bins = 30)

sns.boxplot(data = df, x = 'children', y = 'charges')

df2 = pd.pivot_table(data = df, index = 'age', columns = 'region', values = 'charges', aggfunc = np.mean)

plt.figure(figsize = (9,7), dpi = 120)
sns.heatmap(data = df2)

# Q2. Encoding
# Here we have sex, region and smoker as categorical columns, we can use get_dummies

df1 = pd.get_dummies(data = df, columns = ['sex','smoker','region'], dtype = int)

df1.head()



df.drop_duplicates(inplace = True)
# 1 row is removed

# Removing duplicates
df[df.duplicated()]
# there is one duplicate value

X = df1.drop(columns = {'charges'})
y = df1['charges']

# Q4 - Data Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Q5
linear = LinearRegression()

linear.fit(X_train,y_train)

y_pred = linear.predict(X_test)

# Q6
from sklearn.metrics import mean_squared_error,r2_score

mse = mean_squared_error(y_test,y_pred)
R2 = r2_score(y_test,y_pred)

rmse = np.sqrt(mse)

print('The mean squared error is:',mse)
print('The R square value is:',R2)
print('The root mean squared error is:',rmse)

# The convergence criteria is met when a specifc number of iterations is reached or when the lowest error is provided by the model after multiple such iterations
# In gradient descent, each paramter in the model is updated after each iteration to ensure the reduction in error



