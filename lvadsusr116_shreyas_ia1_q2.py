# -*- coding: utf-8 -*-
"""LVADSUSR116_Shreyas_IA1-Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-HowPBjyd4lc1NxCqm5JBtNGNRJT7kLY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.tree import DecisionTreeClassifier

pd.options.display.max_rows = 1500

df = pd.read_csv('booking.csv')

# Q1
df.head()

df.info()

for i in df.columns:
    print(df[df[i].isnull()])
# No null values are present

df.head(1)

# Lets check outliers for bmi and charges
# average price
Q1 = df['average price'].quantile(0.25)
Q3 = df['average price'].quantile(0.75)

IQR = Q3-Q1

lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR

# Outlier datapoints
df[(df['average price'] < lower_limit) | (df['average price'] > upper_limit)]

df.drop(columns = {'Booking_ID'}, inplace = True)
# here we are dropping Booking ID as that wont help us much in our machine learning model

# Basic visualizations
sns.boxplot(data = df, x = 'market segment type', y= 'average price')

sns.scatterplot(data = df,x = 'lead time', y = 'average price', hue = 'market segment type')

sns.scatterplot(data = df,x = 'lead time', y = 'average price', hue = 'booking status')



# Q2. Encoding
# Here we have room type, meal type,market segment type as categorical columns, we can use get_dummies
df.head()

df2 = pd.get_dummies(data = df, columns = ['type of meal','room type','market segment type'], dtype = int)

df2

df2['booking status'] = df2['booking status'].apply(lambda x: 1 if x == 'Canceled' else 0)
# We are converting booking status into 1 and 0 where 1 - Canceled and 0 - Not canceled

pd.options.display.max_columns = 100

# Here we will remove all features that have a very weak poisitive or negative correlation with booking status(between -0.1 and 0.1)
df3 = df2.corr(numeric_only = True)['booking status'].to_frame()



df3[df3['booking status'].between(-0.1,0.1)].index
# All these columns have low correlation with booking status

df2.drop(columns = {'number of adults', 'number of children', 'number of weekend nights',
       'number of week nights', 'car parking space', 'P-C', 'P-not-C',
       'type of meal_Meal Plan 1', 'type of meal_Meal Plan 2',
       'type of meal_Meal Plan 3', 'type of meal_Not Selected',
       'room type_Room_Type 1', 'room type_Room_Type 2',
       'room type_Room_Type 3', 'room type_Room_Type 4',
       'room type_Room_Type 5', 'room type_Room_Type 6',
       'room type_Room_Type 7', 'market segment type_Aviation',
       'market segment type_Complementary', 'market segment type_Offline'}, inplace = True)

df2



df2[df2.duplicated()]
df2.drop_duplicates(inplace = True)

df2

df2.drop(columns = {'date of reservation'}, inplace = True)

X = df2.drop(columns = {'booking status'})
y = df2['booking status']



# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train

log_model = LogisticRegression()

log_model.fit(X_train,y_train)

y_pred = log_model.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,recall_score

accuracy_score(y_test,y_pred)

recall_score(y_test,y_pred)

classification_report(y_test,y_pred)

confusion_matrix(y_test,y_pred)
# So here we can see that 1496 values havent been predicted properly while the rest have been, thus giving an accuracy score of 0.8



# Sigmoid function is suitable since is range is restricted within 0 and 1 which is helpful during classification problems