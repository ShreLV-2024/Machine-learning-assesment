# -*- coding: utf-8 -*-
"""LVADSUSR116_Shreyas_IA2-Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dO_jz4sOhf42PH-J4V6G2XX7RZBmLfzV
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, f1_score, recall_score, confusion_matrix, silhouette_score, davies_bouldin_score, calinski_harabasz_score
import time
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans

df = pd.read_csv('winequality-red.csv')

df.head()

# Handling missing values and outlied
# Here we can impute the values by mean

for i in df.columns:
    df[i].fillna(np.mean(df[i]), inplace = True)

for i in df.columns:
    print(df[df[i].isnull()])
# No null values left

outliers = {}

for i in df.columns:
    Q1 = df[i].quantile(0.25)
    Q3 = df[i].quantile(0.75)

    IQR = Q3-Q1

    lower_limit = Q1 - 1.5*IQR
    upper_limit = Q3 + 1.5*IQR

    a = df[(df[i] < lower_limit) | (df[i] > upper_limit)]

    outliers[i] = a.count()[0]

outliers
# these are the total numbers of outliers in each column
# But without further domain knowledge, we cant remove the outliers



# Data Transformation

def Quality(quality):
    if quality <=3:
        return 'Very Bad'
    elif quality <=6:
        return 'Bad'
    elif quality <=8:
        return 'Good'
    else:
        return 'Very good'

df['Quality'] = df.apply(lambda x: Quality(x['quality']), axis = 1)

df.drop(columns = {'quality'}, inplace = True)



# Encoding
# Here since all the features are in numerical format, we dont need to do any encoding
label_encoder = LabelEncoder()

df['Quality'] = label_encoder.fit_transform(df['Quality'])

df.Quality.unique()
# here 0 repersents bad, 1 represents good and 2 represents very good

# Feature selection and data cleaning
df1 = df.corr()['Quality'].to_frame()

# Here we can remove all the columns that lie in the range between -0.1 and 0.1

df1[df1['Quality'].between(-0.1,0.1)]
# these are the only not required columns

df1[df1['Quality'].between(-0.1,0.1)].index

df.drop(columns = {'residual sugar', 'chlorides', 'free sulfur dioxide', 'pH'}, inplace = True)

df[df.duplicated()]

df.drop_duplicates(inplace = True)



# Data splitting

X = df.drop(columns = {'Quality'})
y= df['Quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

rf = RandomForestClassifier(n_estimators = 128,max_features = 'log2')

rf.fit(X_train,y_train)

y_pred = rf.predict(X_test)

accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

f1_score(y_test,y_pred)

recall_score(y_test,y_pred)

precision_score(y_test,y_pred)

